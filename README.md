# Curious-Pong
Implementing Machine Learning for Playing Pong with Curiosity Driven Exploration
Deep reinforcement learning is one of the best machine learning algorithms that teaches machines various tasks based on trial and error experiences, quite similar in the way humans learn. Desired actions result in rewards while undesired ones have no rewards associated with them. The model used to implement deep reinforcement for this paper utilized a convolutional neural network (CNN) to determine a Q-function where input is raw pixels and output is a value function estimating future rewards. We applied our method to Pong and a few other Atari games, with no adjustment of the architecture or learning algorithm. However, in many real-world scenarios, rewards for every action is extremely scarce, or sometimes completely absent rendering the deep learning algorithms virtually unusable. In such cases, curiosity as an action serves as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. Three possible settings are investigated: 1) sparse extrinsic rewarding, where curiosity allows the agent to reach the goal with far fewer exploration; 2) exploration with no extraneous reward, where curiosity encourages the agent to explore in a more efficient manner; and 3) generalising  unseen scenarios (e.g. new levels of the same game) where the knowledge gained from an earlier experience helps the agent explore new places much faster than starting from scratch.

Full project with report uploaded in Google Drive : https://drive.google.com/open?id=1eDsEUW7O5Xx3ahWww2M6q6RtSLhtySlm
